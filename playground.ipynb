{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using Statistics\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Matrix{Float64}}:\n",
       " [4.550121595210181 1.700274660121937 … 5.254907657778933 2.083064155509878; 3.8471541142042067 3.1283424181052757 … 3.8738437723677883 2.6841725312646405; … ; 1.8203071153870078 1.3698744445597113 … 2.0898543043652213 0.53601649623376; 4.384254098580098 2.4411220123602884 … 5.715499758606215 2.2778623971890495]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate dataset of NN matricies W and H and their product V\n",
    "#set seed\n",
    "Random.seed!(314)\n",
    "\n",
    "# V ∈ R^(m × n), and r is the (low) rank of V\n",
    "m, n = 15, 15\n",
    "r = 5\n",
    "true_W = abs.(randn((m, r)))\n",
    "\n",
    "train_set_size = 5\n",
    "test_set_size  = 1\n",
    "\n",
    "train_H = [abs.(randn((r, n))) for _ ∈ 1:train_set_size]\n",
    "test_H  = [abs.(randn((r, n))) for _ ∈ 1:test_set_size ]\n",
    "train_V = [true_W*train_H[i]   for i ∈ 1:train_set_size]\n",
    "test_V  = [true_W*test_H[i]    for i ∈ 1:test_set_size ];\n",
    "\n",
    "# LATER can add small noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the model (copied from https://fluxml.ai/Flux.jl/stable/models/advanced/)\n",
    "struct MyLayer\n",
    "    W\n",
    "end\n",
    "\n",
    "function MyLayer(m,r) # TODO add type stable-ness\n",
    "    W = abs.(randn((m,r))) # TODO adjust initialization\n",
    "    MyLayer(W)\n",
    "end\n",
    "\n",
    "function (a::MyLayer)((H, V)) #must pass a single item into chains, that is why (H,V) are grouped\n",
    "    W = a.W\n",
    "    return (H .* (W'*V) ./ (W'*W*H), V) #one update step\n",
    "    # TODO can add regularization later\n",
    "end\n",
    "Flux.trainable(a::MyLayer) = (a.W,)\n",
    "#Flux.@functor MyLayer \n",
    "\n",
    "struct MyModel\n",
    "    chain::Chain\n",
    "end\n",
    "\n",
    "H_guess = abs.(randn((r,n))) # same random guess for all inputs\n",
    "\n",
    "# this is automatic from the struct\n",
    "#function MyModel(chain::Chain)\n",
    "#    MyModel(chain) \n",
    "#end\n",
    "\n",
    "function (m::MyModel)(V::Matrix{Float64}, H_guess=H_guess)\n",
    "    H_out, V_out = m.chain((H_guess, V)) #must pass a single item into chains\n",
    "    return H_out #just care about the first entry\n",
    "end\n",
    "\n",
    "function (m::MyModel)(V::Vector{Matrix{Float64}}) #TODO update Float64 to something more general\n",
    "    return m.(V) #apply m to all elements in V\n",
    "end\n",
    "  \n",
    "# Call @functor to allow for training. Described below in more detail.\n",
    "Flux.@functor MyModel\n",
    "\n",
    "my_layer = MyLayer(m,r)\n",
    "chain = Chain([my_layer, my_layer, my_layer]) #three layer network, each layer shares parameters\n",
    "model(V) = (MyModel(chain))(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[0.8369810917757003 1.9484490853346115 … 1.7028301525984841 1.1849310086642832; 3.284342060186844 0.7522631357129886 … 1.380454609814672 1.6805481979043364; … ; 0.945360251000689 1.140117305034255 … 0.515135349502517 0.5831160684308441; 1.1630795992759264 0.8821387410872338 … 0.7217666823970634 1.0283045175298946]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Flux.params(my_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Matrix{Float64}}:\n",
       " [0.4563601479487161 0.05735049501675889 … 0.37823974090732115 0.04715936995397002; 0.5177975942067371 0.6318323707579199 … 0.4824335929629952 0.4767183550258751; … ; 0.2517072578420455 0.7165398334508908 … 0.3750541894372231 0.5731476511879695; 0.5038194799079283 0.5434531299255546 … 0.9799203856286202 0.2549237596127586]\n",
       " [0.9754424491803148 0.14985080268852766 … 0.3315935188437829 0.04711185170546529; 0.9040101232127244 1.6533579015213202 … 0.2928732321965072 0.42155361940300723; … ; 0.49406585419135735 1.8315940046013917 … 0.29782362752953284 0.6410324400109199; 0.9979864585426028 1.3304055180543664 … 0.9303867630634411 0.3395330899980648]\n",
       " [1.4553100751526253 0.04245870136254515 … 0.4464299800561515 0.09384326041147444; 1.04146143566033 0.6583061701314502 … 0.5210871014468536 1.193077702367736; … ; 0.6420278819506194 0.5390135192242497 … 0.47179915549946844 1.3663261481158055; 1.5017543009099987 0.3980717848945929 … 1.5085902168591232 0.5434871810434522]\n",
       " [1.0971902929762327 0.06554242738716222 … 0.9723639096557934 0.09952152692550852; 1.0744006595197997 0.698578582953172 … 0.6963072723076492 0.7319148699221409; … ; 0.5182649834001396 0.8624543100173439 … 0.8433674497339567 1.0723791974926609; 1.1845118575572504 0.610263377261379 … 2.477622010966831 0.568925783136368]\n",
       " [0.6641165332532517 0.09440556175234967 … 0.3719123431896173 0.12084538791945508; 0.6305503108781418 0.8720413103457498 … 0.512861246022137 0.9944925248697243; … ; 0.337576011935055 0.9895257520650188 … 0.4169090018222752 1.3212170858587446; 0.7451707818009072 0.7698682277918096 … 1.1268658483368084 0.6631422128729296]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = model(train_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Descent(0.1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "using Flux: mse\n",
    "loss(model, V_input, H_truth) = mean(mse.(model(V_input), H_truth)) # mean squared error\n",
    "                                                                    # the outer mean is to handle batches\n",
    "# TODO idealy the model can be applied on batches without needing the elementwise application dot here\n",
    "#loss(model, train_V, train_H)\n",
    "\n",
    "using Flux: train!\n",
    "\n",
    "opt = Descent() #or Adam(), see https://fluxml.ai/Flux.jl/stable/training/optimisers/ for full list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: setup found no trainable parameters in this model\n",
      "└ @ Optimisers C:\\Users\\Nicholas\\.julia\\packages\\Optimisers\\BT5bT\\src\\interface.jl:27\n"
     ]
    }
   ],
   "source": [
    "data = [(train_V, train_H)]\n",
    "train!(loss, model, data, opt) #cant find trainable parameters, TODO need to pas them through MyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxit = 10\n",
    "training_loss = zeros(1,maxit)\n",
    "testing_loss  = zeros(1,maxit)\n",
    "for i ∈ 1:maxit\n",
    "    train!(loss, model, data, opt)\n",
    "    training_loss[i] = loss(model, train_V, train_H)\n",
    "    testing_loss[i]  = loss(model, test_V , test_H )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model performance on the training and testing set\n",
    "plt(i,[training_loss,testing_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe how close the learned weights ̂W and the true W are\n",
    "learned_W = Flux.params(model)\n",
    "difference = abs.(learned_W - true_W)\n",
    "heatmap(difference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
